{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1beb788a-705e-4163-b61e-7a3527912079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym\n",
      "  Downloading gym-0.26.2.tar.gz (721 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m721.7/721.7 kB\u001b[0m \u001b[31m81.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.18.0 in /opt/anaconda3/lib/python3.12/site-packages (from gym) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from gym) (2.2.1)\n",
      "Collecting gym_notices>=0.0.4 (from gym)\n",
      "  Downloading gym_notices-0.0.8-py3-none-any.whl.metadata (1.0 kB)\n",
      "Downloading gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827728 sha256=23ed9276335a749bc3a38224acc9df8177634509490f1c0a99dffc8062550a4a\n",
      "  Stored in directory: /Users/guruprasad/Library/Caches/pip/wheels/95/51/6c/9bb05ebbe7c5cb8171dfaa3611f32622ca4658d53f31c79077\n",
      "Successfully built gym\n",
      "Installing collected packages: gym_notices, gym\n",
      "Successfully installed gym-0.26.2 gym_notices-0.0.8\n"
     ]
    }
   ],
   "source": [
    "!pip install gym\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b3aca36-ee06-40b2-8fd9-de8a9b5f0b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/gym/spaces/box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the agent...\n",
      "Episode 0/500, Reward: 1342.00, Epsilon: 0.99\n",
      "Episode 50/500, Reward: 1031.00, Epsilon: 0.77\n",
      "Episode 100/500, Reward: 1076.00, Epsilon: 0.60\n",
      "Episode 150/500, Reward: 1185.00, Epsilon: 0.47\n",
      "Episode 200/500, Reward: 987.50, Epsilon: 0.37\n",
      "Episode 250/500, Reward: 984.00, Epsilon: 0.28\n",
      "Episode 300/500, Reward: 1052.00, Epsilon: 0.22\n",
      "Episode 350/500, Reward: 1848.50, Epsilon: 0.17\n",
      "Episode 400/500, Reward: 996.50, Epsilon: 0.13\n",
      "Episode 450/500, Reward: 1007.50, Epsilon: 0.10\n",
      "Training complete!\n",
      "\n",
      "Post-training options:\n",
      "Select a country:\n",
      "- USA\n",
      "- India\n",
      "- Canada\n",
      "- UK\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the country name:  India\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Country: India\n",
      "Initial Indoor Temp: 28°C, Outdoor Temp: 39°C\n",
      "Suggested Indoor Temperature: 28°C\n",
      "Outdoor Temperature: 39°C\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you like this temperature? (yes/no):  yes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great! Using the suggested temperature.\n",
      "Final Indoor Temperature: 28°C for India\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Would you like to select another country? (yes/no):  uk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program ended.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from collections import deque\n",
    "\n",
    "# Define climate conditions for different countries\n",
    "country_climate = {\n",
    "    'USA': {'indoor': (20, 25), 'outdoor': (-10, 35)},\n",
    "    'India': {'indoor': (22, 28), 'outdoor': (10, 40)},\n",
    "    'Canada': {'indoor': (18, 24), 'outdoor': (-30, 30)},\n",
    "    'UK': {'indoor': (20, 24), 'outdoor': (-5, 30)},\n",
    "}\n",
    "\n",
    "# Custom Environment for Thermostat\n",
    "class SmartThermostatEnv(gym.Env):\n",
    "    def __init__(self, country='USA', manual_mode=False):\n",
    "        super(SmartThermostatEnv, self).__init__()\n",
    "        \n",
    "        self.observation_space = gym.spaces.Box(low=np.array([10, -10, 0, 0]), \n",
    "                                                high=np.array([35, 40, 2, 1]), dtype=np.float32)\n",
    "        self.action_space = gym.spaces.Discrete(3)\n",
    "        \n",
    "        self.country_defaults = {'USA': 22, 'India': 24, 'Canada': 20, 'UK': 21}\n",
    "        self.country = country\n",
    "        self.target_temp = self.country_defaults.get(country, 22)\n",
    "        \n",
    "        self.manual_mode = manual_mode\n",
    "        self.max_steps = 100\n",
    "        self.current_step = 0\n",
    "    \n",
    "    def step(self, action):\n",
    "        if self.manual_mode:\n",
    "            return self._get_state(), 0, False, {}\n",
    "        \n",
    "        # Apply action: 0 = Decrease, 1 = Maintain, 2 = Increase\n",
    "        if action == 0:\n",
    "            self.indoor_temp -= 1\n",
    "        elif action == 1:\n",
    "            self.indoor_temp += 0\n",
    "        elif action == 2:\n",
    "            self.indoor_temp += 1\n",
    "        \n",
    "        # Bound temperature\n",
    "        self.indoor_temp = max(10, min(35, self.indoor_temp))\n",
    "        \n",
    "        # Calculate reward (always positive)\n",
    "        comfort_reward = 10 - min(10, abs(self.target_temp - self.indoor_temp))\n",
    "        energy_reward = 5 if action == 1 else 4.5\n",
    "        base_reward = 5\n",
    "        reward = comfort_reward + energy_reward + base_reward  # 9.5 to 20\n",
    "        \n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= self.max_steps\n",
    "        return self._get_state(), reward, done, {}\n",
    "    \n",
    "    def reset(self):\n",
    "        indoor_range = country_climate[self.country]['indoor']\n",
    "        outdoor_range = country_climate[self.country]['outdoor']\n",
    "        self.indoor_temp = random.randint(indoor_range[0], indoor_range[1])\n",
    "        self.outdoor_temp = random.randint(outdoor_range[0], outdoor_range[1])\n",
    "        self.time_of_day = random.choice([0, 1, 2])\n",
    "        self.occupied = random.choice([0, 1])\n",
    "        self.current_step = 0\n",
    "        return self._get_state()\n",
    "    \n",
    "    def _get_state(self):\n",
    "        return np.array([self.indoor_temp, self.outdoor_temp, self.time_of_day, self.occupied], dtype=np.float32)\n",
    "\n",
    "# DQN Agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.99\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.0005\n",
    "        self.model = self._build_model()\n",
    "    \n",
    "    def _build_model(self):\n",
    "        model = Sequential([\n",
    "            Dense(64, activation='relu', input_dim=self.state_size),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dense(self.action_size, activation='linear')\n",
    "        ])\n",
    "        model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def act(self, state, train=True):\n",
    "        if train and np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        q_values = self.model.predict(np.array([state]), verbose=0)\n",
    "        return np.argmax(q_values[0])\n",
    "    \n",
    "    def replay(self, batch_size=32):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target += self.gamma * np.amax(self.model.predict(np.array([next_state]), verbose=0)[0])\n",
    "            target_f = self.model.predict(np.array([state]), verbose=0)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(np.array([state]), target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "# Function to select a country\n",
    "def select_country():\n",
    "    print(\"Select a country:\")\n",
    "    for country in country_climate.keys():\n",
    "        print(f\"- {country}\")\n",
    "    \n",
    "    selected_country = input(\"Enter the country name: \").strip()\n",
    "    if selected_country in country_climate:\n",
    "        return selected_country\n",
    "    else:\n",
    "        print(\"Invalid country selected. Defaulting to USA.\")\n",
    "        return 'USA'\n",
    "\n",
    "# Function to generate climate conditions\n",
    "def generate_climate_conditions(country):\n",
    "    indoor_range = country_climate[country]['indoor']\n",
    "    outdoor_range = country_climate[country]['outdoor']\n",
    "    indoor_temp = random.randint(indoor_range[0], indoor_range[1])\n",
    "    outdoor_temp = random.randint(outdoor_range[0], outdoor_range[1])\n",
    "    return indoor_temp, outdoor_temp\n",
    "\n",
    "# Function to get user feedback\n",
    "def get_user_feedback(indoor_temp, outdoor_temp):\n",
    "    print(f\"Suggested Indoor Temperature: {indoor_temp}°C\")\n",
    "    print(f\"Outdoor Temperature: {outdoor_temp}°C\")\n",
    "    feedback = input(\"Do you like this temperature? (yes/no): \").strip().lower()\n",
    "    return feedback == 'yes'\n",
    "\n",
    "# Function for manual temperature setting\n",
    "def manual_temperature_setting(outdoor_temp):\n",
    "    while True:\n",
    "        try:\n",
    "            indoor_temp = float(input(\"Enter your preferred indoor temperature (10-35°C): \"))\n",
    "            if 10 <= indoor_temp <= 35:\n",
    "                break\n",
    "            else:\n",
    "                print(\"Temperature must be between 10°C and 35°C.\")\n",
    "        except ValueError:\n",
    "            print(\"Invalid input. Please enter a number.\")\n",
    "    return indoor_temp, outdoor_temp\n",
    "\n",
    "# Train the Agent and Post-Training Interaction\n",
    "if __name__ == \"__main__\":\n",
    "    # Training Phase\n",
    "    env = SmartThermostatEnv(country='USA')\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "    EPISODES = 500\n",
    "    print(\"Training the agent...\")\n",
    "    for e in range(EPISODES):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.act(state, train=True)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "        agent.replay()\n",
    "        if e % 50 == 0:\n",
    "            print(f\"Episode {e}/{EPISODES}, Reward: {total_reward:.2f}, Epsilon: {agent.epsilon:.2f}\")\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "    # Post-Training Interaction\n",
    "    while True:\n",
    "        print(\"\\nPost-training options:\")\n",
    "        selected_country = select_country()\n",
    "        env = SmartThermostatEnv(country=selected_country)\n",
    "        \n",
    "        # Generate initial climate conditions\n",
    "        indoor_temp, outdoor_temp = generate_climate_conditions(selected_country)\n",
    "        state = np.array([indoor_temp, outdoor_temp, random.choice([0, 1, 2]), random.choice([0, 1])], dtype=np.float32)\n",
    "        \n",
    "        # Use trained agent to suggest temperature\n",
    "        action = agent.act(state, train=False)\n",
    "        if action == 0:\n",
    "            suggested_temp = indoor_temp - 1\n",
    "        elif action == 1:\n",
    "            suggested_temp = indoor_temp\n",
    "        else:\n",
    "            suggested_temp = indoor_temp + 1\n",
    "        \n",
    "        # Bound the suggested temperature\n",
    "        suggested_temp = max(10, min(35, suggested_temp))\n",
    "        \n",
    "        print(f\"\\nCountry: {selected_country}\")\n",
    "        print(f\"Initial Indoor Temp: {indoor_temp}°C, Outdoor Temp: {outdoor_temp}°C\")\n",
    "        \n",
    "        # Get user feedback\n",
    "        if get_user_feedback(suggested_temp, outdoor_temp):\n",
    "            print(\"Great! Using the suggested temperature.\")\n",
    "            final_temp = suggested_temp\n",
    "        else:\n",
    "            print(\"Switching to manual mode...\")\n",
    "            final_temp, outdoor_temp = manual_temperature_setting(outdoor_temp)\n",
    "            print(f\"Manually set temperature: {final_temp}°C\")\n",
    "        \n",
    "        # Display final choice\n",
    "        print(f\"Final Indoor Temperature: {final_temp}°C for {selected_country}\")\n",
    "        \n",
    "        # Ask if user wants to continue\n",
    "        cont = input(\"Would you like to select another country? (yes/no): \").strip().lower()\n",
    "        if cont != 'yes':\n",
    "            break\n",
    "    \n",
    "    print(\"Program ended.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21665107-9e92-4c27-82e0-1e50909890ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
